[
  {
    "artist": "Test Student",
    "instagram": "https://www.instagram.com/teststudent/",
    "website": "https://website.com",
    "vimeo": "https://vimeo.com/test-student",
    "title": "Title",
    "year": "Year",
    "medium": "Medium",
    "materials": "Materials",
    "statement": "Statement",
    "description": "Description",
    "credit": "Credit",
    "media": ["1.jpg", "2.jpg", "3.jpg", "4.mp4", "5.jpg", "6.jpg", "7.jpg", "8.jpg"],
    "include": false
  },
  {
    "artist": "Alyssa Cheng",
    "instagram": "",
    "vimeo": "",
    "website": "",
    "title": "Reconstruction",
    "year": "2021",
    "medium": "Video",
    "materials": "StyleGAN2-ADA Latent Space Interpolation",
    "statement": "Alyssa Cheng is a multidisciplinary artist who works on fashion and materials. In the project, <i>Reconstruction</i>, she explores how fashion could be combined with technologies to develop a new perspective. By using AI technology to flatten the 3D garments, the garments are transformed into changing pixels and into the language of images. This process turns the produced garments back into the designer's computer and becomes a source of inspiration for designing.",
    "description": "There are 2354 images in the dataset. All images in the database are from all Alexander McQueen runway shows from 1995 to 2010. This project uses StyleGAN2-ADA as Machine Learning model, and it is trained in Google Colab.",
    "credit": "Vogue Runway for images involved in the creation of the dataset, Derrick Schultz for the creation of the StyleGAN2-ADA-PyTorch repo/notebook",
    "media": ["alyssa-cheng1.mp4"],
    "include": true
  },
  {
    "artist": "Benjamin Glass",
    "instagram": "https://www.instagram.com/ben_g.lass/",
    "website": "",
    "vimeo": "https://vimeo.com/twdfilms/",
    "title": "Reconstruction",
    "year": "2021",
    "medium": "Single Channel Video, Images",
    "materials": "StyleGAN2-ADA Latent Space Interpolation video and images.",
    "statement": "The <i>Your_Son 2021</i> project explores the latency of age as it appears in synthetic imagery. In this work the artists trained an image generation model from a selection of around 3000 images taken throughout their entire life. After training, the model was capable of generating 500 of its own synthetic images which where organized and cataloged by perceived age by the parents of the artist, creating a second round of image discrimination fascilitated by human processes. The resulting work is a 9 and a half minute video that interpolates between 20 images, an image for each year of the artist’s life.",
    "description": "The dataset is composed of exactly 2,782 images. The majority of images were harvested digitally from social media accounts in which I was featured as well as those existing on hard drives and cameras. Many images were scanned into digital files from family photobooks and physical images in the homes of family. StyleGAN2-ADA, Align_Faces, RunwayML Green Screen, combined training time of 30 hours, through Google Colab, using primarily a Tesla P100 Nvidia GPU, Latent Space interpolation in the “w” space.",
    "credit": "Derrick Shultz for the StyleGAN2-ADA-PyTorch notebook, Doug Rosman for the CVML/Dataset tools repo, Nicole & Kevin Glass.",
    "media": ["Your_Son2021_2.mp4", "ContactSheet-001.jpg", "ContactSheet-002.jpg", "ContactSheet-003.jpg", "ContactSheet-004.jpg", "ContactSheet-005.jpg", "ContactSheet-006.jpg", "ContactSheet-007.jpg", "ContactSheet-008.jpg", "ContactSheet-009.jpg", "ContactSheet-010.jpg", "ContactSheet-011.jpg", "ContactSheet-012.jpg", "ContactSheet-013.jpg", "ContactSheet-014.jpg", "ContactSheet-015.jpg", "ContactSheet-016.jpg", "ContactSheet-017.jpg", "ContactSheet-018.jpg", "ContactSheet-019.jpg", "ContactSheet-020.jpg"],
    "include": true
  },
  {
    "artist": "Boomer Scripps",
    "instagram": "https://www.instagram.com/boomerscrippsdoesnotexist/",
    "website": "https://boomerscrippsdoesnotexist.com/",
    "vimeo": "",
    "title": "Dreaming of Forgotten Memories",
    "year": "2021",
    "medium": "Video",
    "materials": "StyleGAN2-ADA Latent Space Interpolation",
    "statement": "This piece uses StyleGAN2-ADA trained on images of my hometown, Boulder, Colorado. Boulder and the surrounding area has gone through a lot of rapid and traumatic changes over the past year, to the point that it feels divorced form my own memories of it. With this piece I am trying to work through my own fractured memories of my hometown given that I can’t see anymore.",
    "description": "My data set has 43 images. I made all my images over the past ~year and a half with a medium format film camera (Hasselblad 500C/). Trained using NVLab's StyleGAN2-ADA. Latent space interpolation videos generated using circular interpolation from Derrick Schultz' Colab Notebook.",
    "credit": "Derrick Shultz for the StyleGAN2-ADA-PyTorch notebook",
    "media": ["vid_1_1.mp4","vid_2_1.mp4","vid_3_1.mp4","vid_4_1.mp4","vid_5_1.mp4","vid_6_1.mp4","vid_7_1.mp4","vid_8_1.mp4","vid_9_1.mp4","vid_10_1.mp4","vid_11_1.mp4","vid_12_1.mp4","vid_13_1.mp4","vid_14_1.mp4","vid_15_1.mp4"],
    "include": true
  },
  {
    "artist": "Daniella Thach",
    "instagram": "https://www.instagram.com/uglyneon/",
    "website": "https://daniellathach.wixsite.com/artist/",
    "vimeo": "",
    "title": "Dreaming of Forgotten Memories",
    "year": "2021",
    "medium": "Digital video projection, appropriated painting",
    "materials": "StyleGAN2-ADA Latent Space Interpolation",
    "statement": "Daniella Thach is a multidisciplinary artist utilizing old and new lighting technologies. Their diasporic work reconciles with the inevitable loss of heritage through the inability to speak her mother tongue. By putting archaic and contemporary mediums in conversation, she bridges the gap in her cultural identity where linguistic language could not.<br><br>An image that has grown up with me are depictions of apsara dancers in the paintings and wood carvings hung on the walls of my childhood home. My dataset is composed of stills taken from an excerpt of Apsara (1966) of Princess Norrodom Buppha Devi's dance. The film is a Cambodian romantic drama edited, written, and directed by the former King and Cambodia's Head of State, Norodom Sihanouk to counter the negative portrayal of the country he saw in the 1965 British-American film Lord Jim. I have not seen the entire movie but I was entranced by her dance. I rarely see apsara dances being performed but when I do, it is a glamorous spectacle, tinted in nostalgia from a time and place that I was not a part of. My reverence to this dance is distanced, and so came the experimentation to distort this video through algorithmic processes to visually communicate my warped sense of identity as a Cambodian American.",
    "description": "The dataset has about 1600 stills from Buppha Devi's dance. The clip from the film was downloaded from YouTube and the .mp4 file was broken down into frames through ffmpeg. I lost track of how long I trained my model through Google Colab after 16 hours. I used StyleGan2-ADA and this interpolation is a noise loop with a 8.0 diameter using seed 68.",
    "credit": "I want to thank Derrick Schultz for developing the StyleGAN2-ADA Colab Notebook, making machine learning very accessible to newbies like me. I extend this thanks to Doug Rosman and Blake Fall-Conroy for teaching me how to use Derrick's repo and creating a super easy to navigate Colab Notebook. I want to also acknowledge the Devata research organization for preserving the film I pulled my dataset from, and of course for their ongoing preservation of Southeast Asian women in art.",
    "media": ["as-an-apsara-ai.mp4"],
    "include": true
  },
  {
    "artist": "Henry Boeschenstein",
    "instagram": "",
    "website": "",
    "vimeo": "",
    "title": "Data Center (Scanimations)",
    "year": "2021",
    "medium": "Video",
    "materials": "Pix2PixHD",
    "statement": "",
    "description": "",
    "credit": "",
    "warning": "video contains intense flickering/strobing throughout.",
    "media": ["henry1.mp4"],
    "include": true
  },
  {
    "artist": "Jieun Hong",
    "instagram": "",
    "website": "",
    "vimeo": "",
    "title": "Pavilion",
    "year": "2021",
    "medium": "Video, Website",
    "materials": "StyleGAN2-ADA Latent Space Interpolation, 3D objects",
    "statement": "",
    "description": "",
    "credit": "",
    "preContent": "<a href='https://jhong36.github.io/computer_vision/threejs/3d_model/pavilion/' target='_blank'>View interactive 3D model</a>",
    "warning": "",
    "media": ["jieun-hong1.mp4", "Pavilion01.jpg", "Pavilion02.png"],
    "postContent": "",
    "include": true
  },
  {
    "artist": "Keming Li",
    "instagram": "https://www.instagram.com/keming_li_29/",
    "website": "https://www.kemingli.com/wave-ballet",
    "vimeo": "",
    "title": "Wave Ballet",
    "year": "2021",
    "medium": "Video",
    "materials": "StyleGAN2-ADA Latent Space Interpolation",
    "statement": "The work, <i>Wave Ballet</i> is a visual experimentation of dancing human forms and the ever changing sea waves. The artist created several Latent space interpolation videos with StyleGAN2-ADA-PyTorch repo/notebook, by training the data set of wave and ballet together. The figures morph into different dancing gestures, indicating a ghostly presence of time and existence, creating a connection between the dynamic waves and human body.",
    "description": "Around 1200 images in wave data set and 1200 images in ballet data set. The images come from cc0 videos online, extracted with ffmpeg. StyleGAN2-ADA model trained using google colab. Latent space interpolation Technique: Circular Loop (from Derrick Schultz' StyleGAN2-ADA Colab Notebook.",
    "credit": "StyleGAN2-ADA-Pytorch, NVIDIA Corporation, Tero Karras, Janne Hellsten, Public Domain Videos of wave and ballerina from Pixabay (Free for commercial use, No attribution required), Derrick Schultz who created the StyleGAN2-ADA-PyTorch Colab notebook, Music Credit to Lloyd Rodgers - On Questions of Responsibility (Act II) (Public Domain music)",
    "preContent": "",
    "warning": "",
    "media": ["keming-li1.mp4"],
    "postContent": "",
    "include": true
  },
  {
    "artist": "Li Zhu",
    "instagram": "",
    "website": "https://zhuli.info/humandustpix2pix",
    "vimeo": "",
    "title": "HumanDust_Pix2Pix",
    "year": "2021",
    "medium": "Video",
    "materials": "Pix2PixHD",
    "statement": "",
    "description": "The model was trained in Google Colab, with a dataset of about 2000 images extracted from a video created using Unity3D.",
    "credit": "Doug Rosman's pix2pixHD Colab Notebook",
    "preContent": "",
    "warning": "",
    "media": ["li-zhu1.mp4", "li-zhu2.mp4", "li-zhu3.mp4"],
    "postContent": "",
    "include": true
  },
  {
    "artist": "Max Weiss",
    "instagram": "",
    "website": "",
    "vimeo": "",
    "title": "Membrane",
    "year": "2021",
    "medium": "Digital Image",
    "materials": "Pix2PixHD",
    "statement": "This project was, I suppose, inspired by an earlier project of this semester in which we were to photograph 100 (?) images of our own hands in various positions to be contributed to a larger dataset — at that time I was unfamiliar with Scott Eaton's work, which this final product so coincidentally represents.<br><br>My original vision for \"Membrane\" was to incorporate the \"flesh-y\" appearance into Tony Cragg's large body of sculptural works, which are both suitably organic in form and thoroughly documented. However, it was pointed out early on that as interesting as the results may be, the idea would be conceptually shallow with no significant reason for selecting Cragg's work over that of another abstract sculptor.<br><br>I then considered the original dataset in relation to the entire human body. I wanted my dataset to be composed of complete bodies at first, though quickly realized the difficulty in assembling a large quantity of images of nude figures. This led me to hands as an alternative: with their various formal intricacies including both fleshy and structural elements, they might function as a stand-in for the remainder of the human form — from which I was really only interested in the general appearance of flesh to begin with, rather than visibly different features like eyes or hair.<br><br>With the intention of representing the entire human body with only a fraction of its form, it was logical to apply the aesthetics of the fully-trained model to abstract human sculptures — those that are intended to capture the human likeness to some extent without being complete or literal. The current images that reflect this new conceptual direction are two of Henry Moore's sculptures, though I intend to explore its application to the work of other artists in the future.",
    "description": "<i>Membrane</i> is a project that resulted from a sizeable dataset of hands trained in pix2pix (approximately 1,400 images, 2,800 including the corresponding \"edge\" images). After training for 200 epochs, images of isolated sculptures treated with the same edge-detection used for the hands were used to generate \"flesh-y\" versions of themselves, then composited with the original images they were isolated from to suggest a legitimate physical presence.",
    "credit": "",
    "preContent": "",
    "warning": "",
    "media": ["henry-moore.jpg", "henry-moore2.jpg", "nick-bibby.jpg", "tony-cragg.jpg"],
    "captions": ["Henry Moore - <i>Reclining Figure</i>","Henry Moore - <i>Recumbant Figure</i>","Nick Bibby - <i>Torso of Apollo</i>","Tony Cragg - <i>Mixed Emotions</i>"],
    "postContent": "",
    "include": true
  },
  {
    "artist": "Sean Cheng",
    "instagram": "",
    "website": "",
    "vimeo": "",
    "title": "Buddha and Khenpo ",
    "year": "2021",
    "medium": "Video",
    "materials": "Pix2PixHD, StyleGAN2-ADA",
    "statement": "The clothing fashion in Northern Wei comes from the combination of Xianbai (tribal) clothing style and the style of Han Chinese, which involves a lot of drapery and decorative accessories, it is mostly composed of light sheets and robes. This clothing style can be best exemplified by the Buddha statues of that period. The Buddha Art in Northern Wei is considered as one the highest achievement in all sculptural art forms. I try to animate these Buddha art through the motion of Tsultrim Lodro giving a public lecture.",
    "description": "These Buddha sculpture videos are made with 200 photos of Seated Buddha sculpture collected from different museums and Archeological sites. The variety of Buddha statues ranging from Gandhara period to China Northern Wei Dynasty to Ancient Thailand and Japan. The 200 image is the first process in StyleGan2 using linear interpretations space W to generate a 3 minutes long video. Then I extract frames from this video in Pix2PixHD and apply canny edge detection for each frame. I use these frames to train the new model for about 100 epoch. Later I use a video of Khenpo Tsultrim Lodro (the most prominent Tibetan Buddhist scholars, who oversees the education of about 40000 monks) as my test images. I ran it through frames extraction and Canny Edge Detection, Synthesized it and get the results(the second video). The longest process for me is the time spent in training new models in Pix2PixHD. It took me about 4 days to get to a hundred epochs.",
    "credit": "",
    "preContent": "",
    "warning": "videos 2, 3 and 4 contain flickering/strobing",
    "media": ["sean-sg2-1.mp4", "sean-p2p1.mp4", "sean-p2p2.mp4", "sean-p2p3.mp4"],
    "postContent": "",
    "include": true
  },
  {
    "artist": "Victoria Yujin Yang",
    "instagram": "https://www.instagram.com/victoria_yang.art/",
    "website": "",
    "vimeo": "",
    "title": "Illuminated Canvas",
    "year": "2021",
    "medium": "Video",
    "materials": "Pix2PixHD, Plaster Gauze, LED Strip, Arduino",
    "statement": "Project Statement: Illuminated Canvas expresses the idea of using a light source as an artistic medium. The project consists of a trained model of images taken from my BFA light piece (Interwoven Evolution), which in this case acts as the light source. After the model is trained, it gets fed with new images that were taken in different locations and settings (lake, room, swinging flashlight). These new images are generated with the train model leading to the outcome of the light being sequentially drawn out based on the canny edge detections. ",
    "description": "500 images in dataset, images retrieved from my own camera, pix2pixHD, 2 Days to train Model, Google Colab",
    "credit": "",
    "preContent": "",
    "warning": "videos contain flickering imagery throughout",
    "media": ["bfa_one.png", "bfa_two.png", "victoria-yang1.mp4","victoria-yang2.mp4"],
    "postContent": "",
    "include": true
  }
]